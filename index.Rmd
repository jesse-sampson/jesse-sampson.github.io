---
title: "Coursera ML Project"
author: "Jesse Sampson"
date: "Sunday, February 22, 2015"
output: html_document
---

The goal of this submission is to train a machine learning model to predict how well an exercise was done based on readouts from physical sensors. As I have zero domain expertise in exercise science I am using a Random Forest model to make predictions. Because estimating model accuracy on only one training dataset yields biased results, I have employed cross-validation to estimate out-of-sample error. Using Accuracy (the percent of true positives correctly identified combined with true negatives correctly identified), I split the data seven times into training and test sets and averaged the error rates of these different model fits to approximate an out-of-sample error rate, i.e.how the model would perform given a dataset it had never seen before.   

The first and most important step is to get, clean and regularize the data. I am removing all the summary statistics variables (those with exectly 19,216 missing values) as well as highly correlated variables and those with near zero variance, accomplished with the caret package's nearZeroVar and findCorrelation functions. 
```{r,warning=FALSE,message=FALSE}
# download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv")
# download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")
train<-read.csv("pml-training.csv")
test<-read.csv("pml-testing.csv")
library("caret")
library(Hmisc)
bad<-nearZeroVar(train)
train<-train[,-bad]
morebad<-(sapply(train,function(x){sum(is.na(x))}))
morebad<-which(morebad==19216)
train<-train[,-morebad]
train<-train[,-c(1,3,4,5)]
classe<-train$classe
user<-train$user_name
train$classe<-NULL
train$user_name<-NULL
corBad <- findCorrelation(cor(train,method="spearman"), cutoff = .8)
train <- train[,-corBad]
train<-transform(train,classe=classe,user_name=user)
```

Next I fit my model to the training set. Random Forest randomly selects variables for each node, so I am comparing sizes 2, 7 and 23 for this purpose using the mtry parameter. I am also employing 3-fold cross validation to approximate out-of-sample error. I am also centering/scaling the data to make the variables comparable and applying principal components to reduce variables to a set of uncorrelated features that explain 95% of the datset's variance.
```{r, cache=TRUE}
(fit1<-train(classe~.,data=train,method="rf",preProcess=c("center","scale","pca"),trControl=trainControl(method='cv',number=6),tuneGrid=data.frame(mtry=c(2,7,23))))
```

We see that the cross-validated accuracy is quite high.  

Now I apply the same cleaning steps to the test data and predict my results. We hope that the actual error rate will be close to the cross-validated error rate. My results on the submission portion were 19/20 or 95% accuracy, making the cross validated estimate very reasonable.  
```{r} 
test<-test[,-bad]
test<-test[,-morebad]
test<-test[,-c(1,3,4,5)]
user<-test$user_name
test$user_name<-NULL
test <- test[,-corBad]
test<-transform(test,user_name=user)
answers<-predict(fit1,newdata=test)
head(answers)
```
